{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from torch import nn, optim\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 10\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = { \n",
    "  \"AP\": 'Andhra Pradesh',\n",
    "  \"AR\": 'Arunachal Pradesh',\n",
    "  \"AS\": 'Assam',\n",
    "  \"BR\": 'Bihar',\n",
    "  \"CT\": 'Chhattisgarh',\n",
    "  \"GA\": 'Goa',\n",
    "  \"GJ\": 'Gujarat',\n",
    "  \"HR\": 'Haryana',\n",
    "  \"HP\": 'Himachal Pradesh',\n",
    "  \"JH\": 'Jharkhand',\n",
    "  \"KA\": 'Karnataka',\n",
    "  \"KL\": 'Kerala',\n",
    "  \"MP\": 'Madhya Pradesh',\n",
    "  \"MH\": 'Maharashtra',\n",
    "  \"MN\": 'Manipur',\n",
    "  \"ML\": 'Meghalaya',\n",
    "  \"MZ\": 'Mizoram',\n",
    "  \"NL\": 'Nagaland',\n",
    "  \"OR\": 'Odisha',\n",
    "  \"PB\": 'Punjab',\n",
    "  \"RJ\": 'Rajasthan',\n",
    "  \"SK\": 'Sikkim',\n",
    "  \"TN\": 'Tamil Nadu',\n",
    "  \"TG\": 'Telangana',\n",
    "  \"TR\": 'Tripura',\n",
    "  \"UT\": 'Uttarakhand',\n",
    "  \"UP\": 'Uttar Pradesh',\n",
    "  \"WB\": 'West Bengal',\n",
    "  \"AN\": 'Andaman and Nicobar Islands',\n",
    "  \"CH\": 'Chandigarh',\n",
    "  \"DN\": 'Dadra and Nagar Haveli',\n",
    "  \"DD\": 'Daman and Diu',  \n",
    "  \"DL\": 'Delhi',\n",
    "  \"JK\": 'Jammu and Kashmir',\n",
    "  \"LA\": 'Ladakh',\n",
    "  \"LD\": 'Lakshadweep',\n",
    "  \"PY\": 'Puducherry',\n",
    "  \"TT\": 'India',\n",
    "  \"UN\": 'Unassigned',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>TT</th>\n",
       "      <th>AN</th>\n",
       "      <th>AP</th>\n",
       "      <th>AR</th>\n",
       "      <th>AS</th>\n",
       "      <th>BR</th>\n",
       "      <th>CH</th>\n",
       "      <th>CT</th>\n",
       "      <th>DN</th>\n",
       "      <th>...</th>\n",
       "      <th>PB</th>\n",
       "      <th>RJ</th>\n",
       "      <th>SK</th>\n",
       "      <th>TN</th>\n",
       "      <th>TG</th>\n",
       "      <th>TR</th>\n",
       "      <th>UP</th>\n",
       "      <th>UT</th>\n",
       "      <th>WB</th>\n",
       "      <th>UN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14-Mar-20</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-Mar-20</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-Mar-20</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17-Mar-20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-Mar-20</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>18-Aug-20</td>\n",
       "      <td>64999</td>\n",
       "      <td>84</td>\n",
       "      <td>9652</td>\n",
       "      <td>134</td>\n",
       "      <td>2534</td>\n",
       "      <td>3257</td>\n",
       "      <td>89</td>\n",
       "      <td>808</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>1705</td>\n",
       "      <td>1347</td>\n",
       "      <td>20</td>\n",
       "      <td>5709</td>\n",
       "      <td>1682</td>\n",
       "      <td>205</td>\n",
       "      <td>4218</td>\n",
       "      <td>468</td>\n",
       "      <td>3175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>19-Aug-20</td>\n",
       "      <td>69196</td>\n",
       "      <td>75</td>\n",
       "      <td>9742</td>\n",
       "      <td>75</td>\n",
       "      <td>2116</td>\n",
       "      <td>2884</td>\n",
       "      <td>91</td>\n",
       "      <td>752</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>1683</td>\n",
       "      <td>1312</td>\n",
       "      <td>25</td>\n",
       "      <td>5795</td>\n",
       "      <td>1763</td>\n",
       "      <td>236</td>\n",
       "      <td>5076</td>\n",
       "      <td>264</td>\n",
       "      <td>3169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>20-Aug-20</td>\n",
       "      <td>68518</td>\n",
       "      <td>76</td>\n",
       "      <td>9393</td>\n",
       "      <td>116</td>\n",
       "      <td>1735</td>\n",
       "      <td>2451</td>\n",
       "      <td>119</td>\n",
       "      <td>1052</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>1741</td>\n",
       "      <td>1330</td>\n",
       "      <td>58</td>\n",
       "      <td>5986</td>\n",
       "      <td>1724</td>\n",
       "      <td>190</td>\n",
       "      <td>4824</td>\n",
       "      <td>411</td>\n",
       "      <td>3197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>21-Aug-20</td>\n",
       "      <td>68519</td>\n",
       "      <td>67</td>\n",
       "      <td>9544</td>\n",
       "      <td>60</td>\n",
       "      <td>1856</td>\n",
       "      <td>2461</td>\n",
       "      <td>116</td>\n",
       "      <td>873</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>1503</td>\n",
       "      <td>1335</td>\n",
       "      <td>46</td>\n",
       "      <td>5995</td>\n",
       "      <td>1457</td>\n",
       "      <td>256</td>\n",
       "      <td>4905</td>\n",
       "      <td>447</td>\n",
       "      <td>3245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>22-Aug-20</td>\n",
       "      <td>70067</td>\n",
       "      <td>61</td>\n",
       "      <td>10276</td>\n",
       "      <td>97</td>\n",
       "      <td>1560</td>\n",
       "      <td>2238</td>\n",
       "      <td>145</td>\n",
       "      <td>704</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>1316</td>\n",
       "      <td>1310</td>\n",
       "      <td>45</td>\n",
       "      <td>5980</td>\n",
       "      <td>2474</td>\n",
       "      <td>280</td>\n",
       "      <td>5217</td>\n",
       "      <td>483</td>\n",
       "      <td>3232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     TT  AN     AP   AR    AS    BR   CH    CT  DN  ...    PB  \\\n",
       "0    14-Mar-20     81   0      1    0     0     0    0     0   0  ...     1   \n",
       "1    15-Mar-20     27   0      0    0     0     0    0     0   0  ...     0   \n",
       "2    16-Mar-20     15   0      0    0     0     0    0     0   0  ...     0   \n",
       "3    17-Mar-20     11   0      0    0     0     0    0     0   0  ...     0   \n",
       "4    18-Mar-20     37   0      0    0     0     0    0     0   0  ...     1   \n",
       "..         ...    ...  ..    ...  ...   ...   ...  ...   ...  ..  ...   ...   \n",
       "157  18-Aug-20  64999  84   9652  134  2534  3257   89   808  51  ...  1705   \n",
       "158  19-Aug-20  69196  75   9742   75  2116  2884   91   752  37  ...  1683   \n",
       "159  20-Aug-20  68518  76   9393  116  1735  2451  119  1052  34  ...  1741   \n",
       "160  21-Aug-20  68519  67   9544   60  1856  2461  116   873  53  ...  1503   \n",
       "161  22-Aug-20  70067  61  10276   97  1560  2238  145   704  40  ...  1316   \n",
       "\n",
       "       RJ  SK    TN    TG   TR    UP   UT    WB  UN  \n",
       "0       3   0     1     1    0    12    0     0   0  \n",
       "1       1   0     0     2    0     1    0     0   0  \n",
       "2       0   0     0     1    0     0    1     0   0  \n",
       "3       0   0     0     1    0     2    0     1   0  \n",
       "4       3   0     1     8    0     2    1     0   0  \n",
       "..    ...  ..   ...   ...  ...   ...  ...   ...  ..  \n",
       "157  1347  20  5709  1682  205  4218  468  3175   0  \n",
       "158  1312  25  5795  1763  236  5076  264  3169   0  \n",
       "159  1330  58  5986  1724  190  4824  411  3197   0  \n",
       "160  1335  46  5995  1457  256  4905  447  3245   0  \n",
       "161  1310  45  5980  2474  280  5217  483  3232   0  \n",
       "\n",
       "[162 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 2\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "df = pd.read_csv('https://api.covid19india.org/csv/latest/state_wise_daily.csv')\n",
    "confirmed_df = df[df.Status == \"Confirmed\"]\n",
    "confirmed_df = confirmed_df.reset_index(drop = True)\n",
    "confirmed_df = confirmed_df.drop(\"Status\", axis = 1)\n",
    "confirmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_every_city(daily_conf_cases):\n",
    "    daily_conf_cases.index = pd.to_datetime(confirmed_df['Date'])\n",
    "    city = dict[daily_conf_cases.name]\n",
    "    plt.plot(daily_conf_cases, label = city)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(\"Daily confirmed cases for \" + city);\n",
    "    plt.savefig('confirmed_cases_plot/'+ city + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(data, seq_len):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-seq_len-1):\n",
    "        x1 = data[i:(i+seq_len)]\n",
    "        y1 = data[i+seq_len]\n",
    "        x.append(x1)\n",
    "        y.append(y1)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoronaVirusPredictor(nn.Module):\n",
    "\n",
    "  def __init__(self, n_features, n_hidden, seq_len, n_layers=2):\n",
    "    super(CoronaVirusPredictor, self).__init__()\n",
    "\n",
    "    self.n_hidden = n_hidden\n",
    "    self.seq_len = seq_len\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_size=n_features,\n",
    "      hidden_size=n_hidden,\n",
    "      num_layers=n_layers,\n",
    "      dropout=0.5\n",
    "    )\n",
    "\n",
    "    self.linear = nn.Linear(in_features=n_hidden, out_features=1)\n",
    "\n",
    "  def reset_hidden_state(self):\n",
    "    self.hidden = (\n",
    "        torch.zeros(self.n_layers, self.seq_len, self.n_hidden),\n",
    "        torch.zeros(self.n_layers, self.seq_len, self.n_hidden)\n",
    "    )\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    lstm_out, self.hidden = self.lstm(\n",
    "      sequences.view(len(sequences), self.seq_len, -1),\n",
    "      self.hidden\n",
    "    )\n",
    "    last_time_step = \\\n",
    "      lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
    "    y_pred = self.linear(last_time_step)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "  model, \n",
    "  train_data, \n",
    "  train_labels, \n",
    "  test_data=None, \n",
    "  test_labels=None\n",
    "):\n",
    "  loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "  optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  num_epochs = 100\n",
    "\n",
    "  train_hist = np.zeros(num_epochs)\n",
    "  test_hist = np.zeros(num_epochs)\n",
    "\n",
    "  for t in range(num_epochs):\n",
    "    model.reset_hidden_state()\n",
    "\n",
    "    y_pred = model(train_data)\n",
    "\n",
    "    loss = loss_fn(y_pred.float(), train_labels)\n",
    "\n",
    "    if test_data is not None:\n",
    "      with torch.no_grad():\n",
    "        y_test_pred = model(test_data)\n",
    "        test_loss = loss_fn(y_test_pred.float(), test_labels)\n",
    "      test_hist[t] = test_loss.item()\n",
    "\n",
    "      if t % 10 == 0:  \n",
    "        print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\n",
    "    elif t % 10 == 0:\n",
    "      print(f'Epoch {t} train loss: {loss.item()}')\n",
    "\n",
    "    train_hist[t] = loss.item()\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimiser.step()\n",
    "  \n",
    "  return model.eval(), train_hist, test_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(daily_conf_cases) : \n",
    "    test_data_size = 60\n",
    "    city = dict[daily_conf_cases.name]\n",
    "    train_data = daily_conf_cases[:-test_data_size]\n",
    "    test_data = daily_conf_cases[-test_data_size:]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(train_data[:, np.newaxis])\n",
    "    train_data = scaler.transform(train_data[:, np.newaxis])\n",
    "    test_data = scaler.transform(test_data[:, np.newaxis])\n",
    "    seq_len = 5\n",
    "    xtrain, ytrain = create_seq(train_data, seq_len)\n",
    "    xtest, ytest = create_seq(test_data, seq_len)\n",
    "    xtrain = torch.from_numpy(xtrain).float()\n",
    "    ytrain = torch.from_numpy(ytrain).float()\n",
    "    xtest = torch.from_numpy(xtest).float()\n",
    "    ytest = torch.from_numpy(ytest).float()\n",
    "    model = CoronaVirusPredictor(\n",
    "      n_features=1, \n",
    "      n_hidden=512, \n",
    "      seq_len=seq_len, \n",
    "      n_layers=3\n",
    "    )\n",
    "    model, train_hist, test_hist = train_model(\n",
    "      model, \n",
    "      xtrain, \n",
    "      ytrain, \n",
    "      xtest, \n",
    "      ytest\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "      test_seq = xtest[:1]\n",
    "      preds = []\n",
    "      for _ in range(len(xtest)):\n",
    "        y_test_pred = model(test_seq)\n",
    "        pred = torch.flatten(y_test_pred).item()\n",
    "        preds.append(pred)\n",
    "        new_seq = test_seq.numpy().flatten()\n",
    "        new_seq = np.append(new_seq, [pred])\n",
    "        new_seq = new_seq[1:]\n",
    "        test_seq = torch.as_tensor(new_seq).view(1, seq_len, 1).float()\n",
    "        \n",
    "    true_cases = scaler.inverse_transform(\n",
    "    np.expand_dims(ytest.flatten().numpy(), axis=0)\n",
    "    ).flatten()\n",
    "\n",
    "    predicted_cases = scaler.inverse_transform(\n",
    "      np.expand_dims(preds, axis=0)\n",
    "    ).flatten()\n",
    "\n",
    "    scaler = scaler.fit(np.expand_dims(daily_conf_cases, axis=1))\n",
    "    all_data = scaler.transform(np.expand_dims(daily_conf_cases, axis=1))\n",
    "    X_all, y_all = create_seq(all_data, seq_len)\n",
    "    X_all = torch.from_numpy(X_all).float()\n",
    "    y_all = torch.from_numpy(y_all).float()\n",
    "    model = CoronaVirusPredictor(\n",
    "      n_features=1, \n",
    "      n_hidden=512, \n",
    "      seq_len=seq_len, \n",
    "      n_layers=2\n",
    "    )\n",
    "    model, train_hist, _ = train_model(model, X_all, y_all)\n",
    "    DAYS_TO_PREDICT = 50\n",
    "    with torch.no_grad():\n",
    "      test_seq = X_all[:1]\n",
    "      preds = []\n",
    "      for _ in range(DAYS_TO_PREDICT):\n",
    "        y_test_pred = model(test_seq)\n",
    "        pred = torch.flatten(y_test_pred).item()\n",
    "        preds.append(pred)\n",
    "        new_seq = test_seq.numpy().flatten()\n",
    "        new_seq = np.append(new_seq, [pred])\n",
    "        new_seq = new_seq[1:]\n",
    "        test_seq = torch.as_tensor(new_seq).view(1, seq_len, 1).float()\n",
    "    predicted_cases = scaler.inverse_transform(np.expand_dims(preds, axis=0)).flatten()\n",
    "    predicted_index = pd.date_range(\n",
    "      start=daily_conf_cases.index[-1],\n",
    "      periods=DAYS_TO_PREDICT + 1,\n",
    "      closed='right'\n",
    "    )\n",
    "    predicted_cases = pd.Series(data=predicted_cases,index=predicted_index)\n",
    "    plt.plot(daily_conf_cases, label='Historical Daily Cases for ' + city)\n",
    "    plt.plot(predicted_cases, label='Predicted Daily Cases for ' + city)\n",
    "    plt.legend();\n",
    "    plt.savefig('model_output/'+ city + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_cols=len(confirmed_df.axes[1])\n",
    "# for x in range(1,total_cols) :\n",
    "#     daily_conf_cases = confirmed_df.iloc[0:,x]\n",
    "#     run_for_every_city(daily_conf_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 4.251269340515137 test loss: 127.43582153320312\n",
      "Epoch 10 train loss: 3.6781013011932373 test loss: 112.69070434570312\n",
      "Epoch 20 train loss: 3.5740504264831543 test loss: 107.01036071777344\n",
      "Epoch 30 train loss: 3.371168613433838 test loss: 55.20926284790039\n",
      "Epoch 40 train loss: 3.0252108573913574 test loss: 58.1048698425293\n",
      "Epoch 50 train loss: 2.3194124698638916 test loss: 72.12226867675781\n",
      "Epoch 60 train loss: 1.9592301845550537 test loss: 45.89516830444336\n",
      "Epoch 70 train loss: 1.8852767944335938 test loss: 57.142948150634766\n",
      "Epoch 80 train loss: 1.7876957654953003 test loss: 47.623504638671875\n",
      "Epoch 90 train loss: 2.1121959686279297 test loss: 51.62776184082031\n",
      "Epoch 0 train loss: 10.73287296295166\n",
      "Epoch 10 train loss: 7.356053829193115\n",
      "Epoch 20 train loss: 14.772233963012695\n",
      "Epoch 30 train loss: 7.484115123748779\n",
      "Epoch 40 train loss: 7.295403003692627\n",
      "Epoch 50 train loss: 6.2574663162231445\n",
      "Epoch 60 train loss: 5.665953159332275\n",
      "Epoch 70 train loss: 5.628957748413086\n",
      "Epoch 80 train loss: 3.1036837100982666\n",
      "Epoch 90 train loss: 2.8544797897338867\n",
      "Epoch 0 train loss: 2.8962104320526123 test loss: 32.47897720336914\n",
      "Epoch 10 train loss: 1.7277944087982178 test loss: 23.77791404724121\n",
      "Epoch 20 train loss: 1.698736548423767 test loss: 24.68792724609375\n",
      "Epoch 30 train loss: 1.592088222503662 test loss: 33.29909133911133\n",
      "Epoch 40 train loss: 1.1909425258636475 test loss: 26.681617736816406\n",
      "Epoch 50 train loss: 0.892313539981842 test loss: 27.289554595947266\n",
      "Epoch 60 train loss: 0.8954670429229736 test loss: 27.99970817565918\n",
      "Epoch 70 train loss: 0.7816252112388611 test loss: 25.555633544921875\n",
      "Epoch 80 train loss: 0.7720739245414734 test loss: 26.187471389770508\n",
      "Epoch 90 train loss: 0.7751062512397766 test loss: 29.12436866760254\n",
      "Epoch 0 train loss: 26.359651565551758\n",
      "Epoch 10 train loss: 12.722795486450195\n",
      "Epoch 20 train loss: 12.565336227416992\n",
      "Epoch 30 train loss: 12.481413841247559\n",
      "Epoch 40 train loss: 12.095973014831543\n",
      "Epoch 50 train loss: 12.69372844696045\n",
      "Epoch 60 train loss: 12.53474235534668\n",
      "Epoch 70 train loss: 12.709704399108887\n",
      "Epoch 80 train loss: 12.611367225646973\n",
      "Epoch 90 train loss: 12.474715232849121\n",
      "Epoch 0 train loss: 4.623269557952881 test loss: 527.6085815429688\n",
      "Epoch 10 train loss: 3.08052921295166 test loss: 489.3104248046875\n",
      "Epoch 20 train loss: 2.71260404586792 test loss: 375.6929931640625\n",
      "Epoch 30 train loss: 1.9190200567245483 test loss: 184.13885498046875\n",
      "Epoch 40 train loss: 1.7206236124038696 test loss: 151.84246826171875\n",
      "Epoch 50 train loss: 1.5421794652938843 test loss: 196.23382568359375\n",
      "Epoch 60 train loss: 1.4741499423980713 test loss: 174.55763244628906\n",
      "Epoch 70 train loss: 1.5182007551193237 test loss: 164.2718048095703\n",
      "Epoch 80 train loss: 1.5507471561431885 test loss: 170.2515106201172\n",
      "Epoch 90 train loss: 1.5030944347381592 test loss: 174.82131958007812\n",
      "Epoch 0 train loss: 10.39700698852539\n",
      "Epoch 10 train loss: 5.350655555725098\n",
      "Epoch 20 train loss: 8.053579330444336\n",
      "Epoch 30 train loss: 7.569168567657471\n",
      "Epoch 40 train loss: 3.585832118988037\n",
      "Epoch 50 train loss: 4.640368938446045\n",
      "Epoch 60 train loss: 2.4125170707702637\n",
      "Epoch 70 train loss: 1.9142887592315674\n",
      "Epoch 80 train loss: 1.6214207410812378\n",
      "Epoch 90 train loss: 1.318050742149353\n",
      "Epoch 0 train loss: 8.123031616210938 test loss: 5339.8759765625\n",
      "Epoch 10 train loss: 5.371035099029541 test loss: 5202.921875\n",
      "Epoch 20 train loss: 5.430479049682617 test loss: 5254.25244140625\n",
      "Epoch 30 train loss: 5.390015602111816 test loss: 5316.4443359375\n",
      "Epoch 40 train loss: 4.33327054977417 test loss: 5342.48876953125\n",
      "Epoch 50 train loss: 7.19923210144043 test loss: 4979.111328125\n",
      "Epoch 60 train loss: 5.625874042510986 test loss: 5266.71728515625\n",
      "Epoch 70 train loss: 5.387698173522949 test loss: 5457.921875\n",
      "Epoch 80 train loss: 5.4479265213012695 test loss: 5494.42724609375\n",
      "Epoch 90 train loss: 5.438604354858398 test loss: 5503.1630859375\n",
      "Epoch 0 train loss: 16.046340942382812\n",
      "Epoch 10 train loss: 11.376643180847168\n",
      "Epoch 20 train loss: 10.854632377624512\n",
      "Epoch 30 train loss: 11.043278694152832\n",
      "Epoch 40 train loss: 8.19300365447998\n",
      "Epoch 50 train loss: 6.240597248077393\n",
      "Epoch 60 train loss: 6.2546162605285645\n",
      "Epoch 70 train loss: 5.704768657684326\n",
      "Epoch 80 train loss: 3.7368855476379395\n",
      "Epoch 90 train loss: 2.3659913539886475\n",
      "Epoch 0 train loss: 11.305196762084961 test loss: 3108.68603515625\n",
      "Epoch 10 train loss: 6.84262752532959 test loss: 2972.6435546875\n",
      "Epoch 20 train loss: 6.809088230133057 test loss: 2975.95654296875\n",
      "Epoch 30 train loss: 6.838293552398682 test loss: 2949.771484375\n",
      "Epoch 40 train loss: 5.725983142852783 test loss: 2608.083740234375\n",
      "Epoch 50 train loss: 6.194123268127441 test loss: 2122.5771484375\n",
      "Epoch 60 train loss: 6.680145263671875 test loss: 3182.540283203125\n",
      "Epoch 70 train loss: 6.435003280639648 test loss: 2970.598388671875\n",
      "Epoch 80 train loss: 5.695319175720215 test loss: 2809.4794921875\n",
      "Epoch 90 train loss: 2.9860167503356934 test loss: 3640.398681640625\n",
      "Epoch 0 train loss: 11.317826271057129\n",
      "Epoch 10 train loss: 7.264331340789795\n",
      "Epoch 20 train loss: 3.7429592609405518\n",
      "Epoch 30 train loss: 1.8091529607772827\n",
      "Epoch 40 train loss: 1.3494036197662354\n",
      "Epoch 50 train loss: 1.1002053022384644\n",
      "Epoch 60 train loss: 1.3051444292068481\n",
      "Epoch 70 train loss: 0.9645865559577942\n",
      "Epoch 80 train loss: 0.937808096408844\n",
      "Epoch 90 train loss: 0.6457018256187439\n",
      "Epoch 0 train loss: 2.361521005630493 test loss: 1.2325024604797363\n",
      "Epoch 10 train loss: 2.0457043647766113 test loss: 0.4905831813812256\n",
      "Epoch 20 train loss: 1.773768424987793 test loss: 1.2305786609649658\n",
      "Epoch 30 train loss: 1.5554447174072266 test loss: 0.3754393756389618\n",
      "Epoch 40 train loss: 1.4801220893859863 test loss: 0.5969099998474121\n",
      "Epoch 50 train loss: 1.4268335103988647 test loss: 0.6025158762931824\n",
      "Epoch 60 train loss: 1.443596363067627 test loss: 0.6228500008583069\n",
      "Epoch 70 train loss: 1.4655208587646484 test loss: 0.6947146058082581\n",
      "Epoch 80 train loss: 1.3839600086212158 test loss: 0.7227798104286194\n",
      "Epoch 90 train loss: 1.3778024911880493 test loss: 0.6751312017440796\n",
      "Epoch 0 train loss: 2.943248748779297\n",
      "Epoch 10 train loss: 2.705277681350708\n",
      "Epoch 20 train loss: 2.677779197692871\n",
      "Epoch 30 train loss: 2.598893642425537\n",
      "Epoch 40 train loss: 2.511899471282959\n",
      "Epoch 50 train loss: 2.378836154937744\n",
      "Epoch 60 train loss: 2.280949592590332\n",
      "Epoch 70 train loss: 2.1951663494110107\n",
      "Epoch 80 train loss: 2.1696276664733887\n",
      "Epoch 90 train loss: 2.1790475845336914\n",
      "Epoch 0 train loss: 0.024331912398338318 test loss: 0.013427222147583961\n",
      "Epoch 10 train loss: 0.05345539003610611 test loss: 0.03355976566672325\n",
      "Epoch 20 train loss: 0.0009680231451056898 test loss: 0.0003894591354764998\n",
      "Epoch 30 train loss: 0.0002982759615406394 test loss: 0.00016152045282069594\n",
      "Epoch 40 train loss: 0.00040720539982430637 test loss: 0.00017888445290736854\n",
      "Epoch 50 train loss: 0.00032973464112728834 test loss: 0.00019512705330271274\n",
      "Epoch 60 train loss: 0.00038317590951919556 test loss: 0.00022487816750071943\n",
      "Epoch 70 train loss: 0.0003197502519469708 test loss: 0.00013467656390275806\n",
      "Epoch 80 train loss: 0.00025171355810016394 test loss: 0.00012247070844750851\n",
      "Epoch 90 train loss: 0.00018397928215563297 test loss: 0.00010893078433582559\n",
      "Epoch 0 train loss: 0.011745317839086056\n",
      "Epoch 10 train loss: 0.10194095224142075\n",
      "Epoch 20 train loss: 0.02750304527580738\n",
      "Epoch 30 train loss: 0.009598387405276299\n",
      "Epoch 40 train loss: 0.002854536985978484\n",
      "Epoch 50 train loss: 0.0007761261658743024\n",
      "Epoch 60 train loss: 0.00022095612075645477\n",
      "Epoch 70 train loss: 0.00011054697097279131\n",
      "Epoch 80 train loss: 0.0001009970364975743\n",
      "Epoch 90 train loss: 9.936529386322945e-05\n",
      "Epoch 0 train loss: 27.814075469970703 test loss: 350.0323486328125\n",
      "Epoch 10 train loss: 16.942516326904297 test loss: 313.69049072265625\n",
      "Epoch 20 train loss: 7.679191589355469 test loss: 225.0926055908203\n",
      "Epoch 30 train loss: 8.15324592590332 test loss: 259.067626953125\n",
      "Epoch 40 train loss: 7.5753302574157715 test loss: 234.3180694580078\n",
      "Epoch 50 train loss: 7.432373046875 test loss: 252.34140014648438\n",
      "Epoch 60 train loss: 7.437264919281006 test loss: 246.989013671875\n",
      "Epoch 70 train loss: 7.504449844360352 test loss: 255.80517578125\n",
      "Epoch 80 train loss: 7.1515045166015625 test loss: 256.0498046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 train loss: 7.446242332458496 test loss: 260.8211669921875\n",
      "Epoch 0 train loss: 18.76854133605957\n",
      "Epoch 10 train loss: 10.074174880981445\n",
      "Epoch 20 train loss: 9.218064308166504\n",
      "Epoch 30 train loss: 8.957684516906738\n",
      "Epoch 40 train loss: 10.983050346374512\n",
      "Epoch 50 train loss: 10.263555526733398\n",
      "Epoch 60 train loss: 9.620718002319336\n",
      "Epoch 70 train loss: 9.417646408081055\n",
      "Epoch 80 train loss: 9.158206939697266\n",
      "Epoch 90 train loss: 7.839008808135986\n",
      "Epoch 0 train loss: 20.59468650817871 test loss: 312.4052734375\n",
      "Epoch 10 train loss: 10.14889144897461 test loss: 193.70252990722656\n",
      "Epoch 20 train loss: 7.948349475860596 test loss: 19.655115127563477\n",
      "Epoch 30 train loss: 13.161867141723633 test loss: 70.18724060058594\n",
      "Epoch 40 train loss: 9.949705123901367 test loss: 211.75047302246094\n",
      "Epoch 50 train loss: 9.97968864440918 test loss: 197.6660919189453\n",
      "Epoch 60 train loss: 9.116024017333984 test loss: 115.96434020996094\n",
      "Epoch 70 train loss: 49.62544250488281 test loss: 20.754114151000977\n",
      "Epoch 80 train loss: 10.863851547241211 test loss: 250.288330078125\n",
      "Epoch 90 train loss: 10.727555274963379 test loss: 296.3848876953125\n",
      "Epoch 0 train loss: 27.565244674682617\n",
      "Epoch 10 train loss: 12.367947578430176\n",
      "Epoch 20 train loss: 11.006189346313477\n",
      "Epoch 30 train loss: 7.775425910949707\n",
      "Epoch 40 train loss: 12.95286750793457\n",
      "Epoch 50 train loss: 13.279221534729004\n",
      "Epoch 60 train loss: 12.00172233581543\n",
      "Epoch 70 train loss: 12.024755477905273\n",
      "Epoch 80 train loss: 11.947418212890625\n",
      "Epoch 90 train loss: 11.761850357055664\n",
      "Epoch 0 train loss: 5.735372066497803 test loss: 45.88650131225586\n",
      "Epoch 10 train loss: 4.009294033050537 test loss: 31.584970474243164\n",
      "Epoch 20 train loss: 3.935913324356079 test loss: 29.36726188659668\n",
      "Epoch 30 train loss: 3.891075372695923 test loss: 26.60539436340332\n",
      "Epoch 40 train loss: 2.717449903488159 test loss: 200.0819091796875\n",
      "Epoch 50 train loss: 4.0748090744018555 test loss: 31.617630004882812\n",
      "Epoch 60 train loss: 3.945819616317749 test loss: 30.856624603271484\n",
      "Epoch 70 train loss: 3.480346441268921 test loss: 19.77430534362793\n",
      "Epoch 80 train loss: 3.231794595718384 test loss: 16.005756378173828\n",
      "Epoch 90 train loss: 2.631835460662842 test loss: 262.280517578125\n",
      "Epoch 0 train loss: 8.317440032958984\n",
      "Epoch 10 train loss: 5.0218048095703125\n",
      "Epoch 20 train loss: 8.119114875793457\n"
     ]
    }
   ],
   "source": [
    "total_cols=len(confirmed_df.axes[1])\n",
    "for x in range(15,total_cols) :\n",
    "    daily_conf_cases = confirmed_df.iloc[0:,x]\n",
    "    daily_conf_cases.index = pd.to_datetime(confirmed_df['Date'])\n",
    "    run_model(daily_conf_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
