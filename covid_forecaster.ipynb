{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from torch import nn, optim\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 10\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = { \n",
    "  \"AP\": 'Andhra Pradesh',\n",
    "  \"AR\": 'Arunachal Pradesh',\n",
    "  \"AS\": 'Assam',\n",
    "  \"BR\": 'Bihar',\n",
    "  \"CT\": 'Chhattisgarh',\n",
    "  \"GA\": 'Goa',\n",
    "  \"GJ\": 'Gujarat',\n",
    "  \"HR\": 'Haryana',\n",
    "  \"HP\": 'Himachal Pradesh',\n",
    "  \"JH\": 'Jharkhand',\n",
    "  \"KA\": 'Karnataka',\n",
    "  \"KL\": 'Kerala',\n",
    "  \"MP\": 'Madhya Pradesh',\n",
    "  \"MH\": 'Maharashtra',\n",
    "  \"MN\": 'Manipur',\n",
    "  \"ML\": 'Meghalaya',\n",
    "  \"MZ\": 'Mizoram',\n",
    "  \"NL\": 'Nagaland',\n",
    "  \"OR\": 'Odisha',\n",
    "  \"PB\": 'Punjab',\n",
    "  \"RJ\": 'Rajasthan',\n",
    "  \"SK\": 'Sikkim',\n",
    "  \"TN\": 'Tamil Nadu',\n",
    "  \"TG\": 'Telangana',\n",
    "  \"TR\": 'Tripura',\n",
    "  \"UT\": 'Uttarakhand',\n",
    "  \"UP\": 'Uttar Pradesh',\n",
    "  \"WB\": 'West Bengal',\n",
    "  \"AN\": 'Andaman and Nicobar Islands',\n",
    "  \"CH\": 'Chandigarh',\n",
    "  \"DN\": 'Dadra and Nagar Haveli',\n",
    "  \"DD\": 'Daman and Diu',  \n",
    "  \"DL\": 'Delhi',\n",
    "  \"JK\": 'Jammu and Kashmir',\n",
    "  \"LA\": 'Ladakh',\n",
    "  \"LD\": 'Lakshadweep',\n",
    "  \"PY\": 'Puducherry',\n",
    "  \"TT\": 'India',\n",
    "  \"UN\": 'Unassigned',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>TT</th>\n",
       "      <th>AN</th>\n",
       "      <th>AP</th>\n",
       "      <th>AR</th>\n",
       "      <th>AS</th>\n",
       "      <th>BR</th>\n",
       "      <th>CH</th>\n",
       "      <th>CT</th>\n",
       "      <th>DN</th>\n",
       "      <th>...</th>\n",
       "      <th>PB</th>\n",
       "      <th>RJ</th>\n",
       "      <th>SK</th>\n",
       "      <th>TN</th>\n",
       "      <th>TG</th>\n",
       "      <th>TR</th>\n",
       "      <th>UP</th>\n",
       "      <th>UT</th>\n",
       "      <th>WB</th>\n",
       "      <th>UN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14-Mar-20</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-Mar-20</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-Mar-20</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17-Mar-20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-Mar-20</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>18-Aug-20</td>\n",
       "      <td>64999</td>\n",
       "      <td>84</td>\n",
       "      <td>9652</td>\n",
       "      <td>134</td>\n",
       "      <td>2534</td>\n",
       "      <td>3257</td>\n",
       "      <td>89</td>\n",
       "      <td>808</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>1705</td>\n",
       "      <td>1347</td>\n",
       "      <td>20</td>\n",
       "      <td>5709</td>\n",
       "      <td>1682</td>\n",
       "      <td>205</td>\n",
       "      <td>4218</td>\n",
       "      <td>468</td>\n",
       "      <td>3175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>19-Aug-20</td>\n",
       "      <td>69196</td>\n",
       "      <td>75</td>\n",
       "      <td>9742</td>\n",
       "      <td>75</td>\n",
       "      <td>2116</td>\n",
       "      <td>2884</td>\n",
       "      <td>91</td>\n",
       "      <td>752</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>1683</td>\n",
       "      <td>1312</td>\n",
       "      <td>25</td>\n",
       "      <td>5795</td>\n",
       "      <td>1763</td>\n",
       "      <td>236</td>\n",
       "      <td>5076</td>\n",
       "      <td>264</td>\n",
       "      <td>3169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>20-Aug-20</td>\n",
       "      <td>68518</td>\n",
       "      <td>76</td>\n",
       "      <td>9393</td>\n",
       "      <td>116</td>\n",
       "      <td>1735</td>\n",
       "      <td>2451</td>\n",
       "      <td>119</td>\n",
       "      <td>1052</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>1741</td>\n",
       "      <td>1330</td>\n",
       "      <td>58</td>\n",
       "      <td>5986</td>\n",
       "      <td>1724</td>\n",
       "      <td>190</td>\n",
       "      <td>4824</td>\n",
       "      <td>411</td>\n",
       "      <td>3197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>21-Aug-20</td>\n",
       "      <td>68519</td>\n",
       "      <td>67</td>\n",
       "      <td>9544</td>\n",
       "      <td>60</td>\n",
       "      <td>1856</td>\n",
       "      <td>2461</td>\n",
       "      <td>116</td>\n",
       "      <td>873</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>1503</td>\n",
       "      <td>1335</td>\n",
       "      <td>46</td>\n",
       "      <td>5995</td>\n",
       "      <td>1457</td>\n",
       "      <td>256</td>\n",
       "      <td>4905</td>\n",
       "      <td>447</td>\n",
       "      <td>3245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>22-Aug-20</td>\n",
       "      <td>70067</td>\n",
       "      <td>61</td>\n",
       "      <td>10276</td>\n",
       "      <td>97</td>\n",
       "      <td>1560</td>\n",
       "      <td>2238</td>\n",
       "      <td>145</td>\n",
       "      <td>704</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>1316</td>\n",
       "      <td>1310</td>\n",
       "      <td>45</td>\n",
       "      <td>5980</td>\n",
       "      <td>2474</td>\n",
       "      <td>280</td>\n",
       "      <td>5217</td>\n",
       "      <td>483</td>\n",
       "      <td>3232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     TT  AN     AP   AR    AS    BR   CH    CT  DN  ...    PB  \\\n",
       "0    14-Mar-20     81   0      1    0     0     0    0     0   0  ...     1   \n",
       "1    15-Mar-20     27   0      0    0     0     0    0     0   0  ...     0   \n",
       "2    16-Mar-20     15   0      0    0     0     0    0     0   0  ...     0   \n",
       "3    17-Mar-20     11   0      0    0     0     0    0     0   0  ...     0   \n",
       "4    18-Mar-20     37   0      0    0     0     0    0     0   0  ...     1   \n",
       "..         ...    ...  ..    ...  ...   ...   ...  ...   ...  ..  ...   ...   \n",
       "157  18-Aug-20  64999  84   9652  134  2534  3257   89   808  51  ...  1705   \n",
       "158  19-Aug-20  69196  75   9742   75  2116  2884   91   752  37  ...  1683   \n",
       "159  20-Aug-20  68518  76   9393  116  1735  2451  119  1052  34  ...  1741   \n",
       "160  21-Aug-20  68519  67   9544   60  1856  2461  116   873  53  ...  1503   \n",
       "161  22-Aug-20  70067  61  10276   97  1560  2238  145   704  40  ...  1316   \n",
       "\n",
       "       RJ  SK    TN    TG   TR    UP   UT    WB  UN  \n",
       "0       3   0     1     1    0    12    0     0   0  \n",
       "1       1   0     0     2    0     1    0     0   0  \n",
       "2       0   0     0     1    0     0    1     0   0  \n",
       "3       0   0     0     1    0     2    0     1   0  \n",
       "4       3   0     1     8    0     2    1     0   0  \n",
       "..    ...  ..   ...   ...  ...   ...  ...   ...  ..  \n",
       "157  1347  20  5709  1682  205  4218  468  3175   0  \n",
       "158  1312  25  5795  1763  236  5076  264  3169   0  \n",
       "159  1330  58  5986  1724  190  4824  411  3197   0  \n",
       "160  1335  46  5995  1457  256  4905  447  3245   0  \n",
       "161  1310  45  5980  2474  280  5217  483  3232   0  \n",
       "\n",
       "[162 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 2\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "df = pd.read_csv('https://api.covid19india.org/csv/latest/state_wise_daily.csv')\n",
    "confirmed_df = df[df.Status == \"Confirmed\"]\n",
    "confirmed_df = confirmed_df.reset_index(drop = True)\n",
    "confirmed_df = confirmed_df.drop(\"Status\", axis = 1)\n",
    "confirmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_every_city(daily_conf_cases):\n",
    "    daily_conf_cases.index = pd.to_datetime(confirmed_df['Date'])\n",
    "    city = dict[daily_conf_cases.name]\n",
    "    plt.plot(daily_conf_cases, label = city)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(\"Daily confirmed cases for \" + city);\n",
    "    plt.savefig('confirmed_cases_plot/'+ city + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(data, seq_len):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-seq_len-1):\n",
    "        x1 = data[i:(i+seq_len)]\n",
    "        y1 = data[i+seq_len]\n",
    "        x.append(x1)\n",
    "        y.append(y1)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoronaVirusPredictor(nn.Module):\n",
    "\n",
    "  def __init__(self, n_features, n_hidden, seq_len, n_layers=2):\n",
    "    super(CoronaVirusPredictor, self).__init__()\n",
    "\n",
    "    self.n_hidden = n_hidden\n",
    "    self.seq_len = seq_len\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_size=n_features,\n",
    "      hidden_size=n_hidden,\n",
    "      num_layers=n_layers,\n",
    "      dropout=0.5\n",
    "    )\n",
    "\n",
    "    self.linear = nn.Linear(in_features=n_hidden, out_features=1)\n",
    "\n",
    "  def reset_hidden_state(self):\n",
    "    self.hidden = (\n",
    "        torch.zeros(self.n_layers, self.seq_len, self.n_hidden),\n",
    "        torch.zeros(self.n_layers, self.seq_len, self.n_hidden)\n",
    "    )\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    lstm_out, self.hidden = self.lstm(\n",
    "      sequences.view(len(sequences), self.seq_len, -1),\n",
    "      self.hidden\n",
    "    )\n",
    "    last_time_step = \\\n",
    "      lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
    "    y_pred = self.linear(last_time_step)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "  model, \n",
    "  train_data, \n",
    "  train_labels, \n",
    "  test_data=None, \n",
    "  test_labels=None\n",
    "):\n",
    "  loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "  optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  num_epochs = 100\n",
    "\n",
    "  train_hist = np.zeros(num_epochs)\n",
    "  test_hist = np.zeros(num_epochs)\n",
    "\n",
    "  for t in range(num_epochs):\n",
    "    model.reset_hidden_state()\n",
    "\n",
    "    y_pred = model(train_data)\n",
    "\n",
    "    loss = loss_fn(y_pred.float(), train_labels)\n",
    "\n",
    "    if test_data is not None:\n",
    "      with torch.no_grad():\n",
    "        y_test_pred = model(test_data)\n",
    "        test_loss = loss_fn(y_test_pred.float(), test_labels)\n",
    "      test_hist[t] = test_loss.item()\n",
    "\n",
    "      if t % 10 == 0:  \n",
    "        print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\n",
    "    elif t % 10 == 0:\n",
    "      print(f'Epoch {t} train loss: {loss.item()}')\n",
    "\n",
    "    train_hist[t] = loss.item()\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimiser.step()\n",
    "  \n",
    "  return model.eval(), train_hist, test_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(daily_conf_cases) : \n",
    "    test_data_size = 60\n",
    "    city = dict[daily_conf_cases.name]\n",
    "    train_data = daily_conf_cases[:-test_data_size]\n",
    "    test_data = daily_conf_cases[-test_data_size:]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(train_data[:, np.newaxis])\n",
    "    train_data = scaler.transform(train_data[:, np.newaxis])\n",
    "    test_data = scaler.transform(test_data[:, np.newaxis])\n",
    "    seq_len = 5\n",
    "    xtrain, ytrain = create_seq(train_data, seq_len)\n",
    "    xtest, ytest = create_seq(test_data, seq_len)\n",
    "    xtrain = torch.from_numpy(xtrain).float()\n",
    "    ytrain = torch.from_numpy(ytrain).float()\n",
    "    xtest = torch.from_numpy(xtest).float()\n",
    "    ytest = torch.from_numpy(ytest).float()\n",
    "    model = CoronaVirusPredictor(\n",
    "      n_features=1, \n",
    "      n_hidden=512, \n",
    "      seq_len=seq_len, \n",
    "      n_layers=3\n",
    "    )\n",
    "    model, train_hist, test_hist = train_model(\n",
    "      model, \n",
    "      xtrain, \n",
    "      ytrain, \n",
    "      xtest, \n",
    "      ytest\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "      test_seq = xtest[:1]\n",
    "      preds = []\n",
    "      for _ in range(len(xtest)):\n",
    "        y_test_pred = model(test_seq)\n",
    "        pred = torch.flatten(y_test_pred).item()\n",
    "        preds.append(pred)\n",
    "        new_seq = test_seq.numpy().flatten()\n",
    "        new_seq = np.append(new_seq, [pred])\n",
    "        new_seq = new_seq[1:]\n",
    "        test_seq = torch.as_tensor(new_seq).view(1, seq_len, 1).float()\n",
    "        \n",
    "    true_cases = scaler.inverse_transform(\n",
    "    np.expand_dims(ytest.flatten().numpy(), axis=0)\n",
    "    ).flatten()\n",
    "\n",
    "    predicted_cases = scaler.inverse_transform(\n",
    "      np.expand_dims(preds, axis=0)\n",
    "    ).flatten()\n",
    "\n",
    "    scaler = scaler.fit(np.expand_dims(daily_conf_cases, axis=1))\n",
    "    all_data = scaler.transform(np.expand_dims(daily_conf_cases, axis=1))\n",
    "    X_all, y_all = create_seq(all_data, seq_len)\n",
    "    X_all = torch.from_numpy(X_all).float()\n",
    "    y_all = torch.from_numpy(y_all).float()\n",
    "    model = CoronaVirusPredictor(\n",
    "      n_features=1, \n",
    "      n_hidden=512, \n",
    "      seq_len=seq_len, \n",
    "      n_layers=2\n",
    "    )\n",
    "    model, train_hist, _ = train_model(model, X_all, y_all)\n",
    "    DAYS_TO_PREDICT = 50\n",
    "    with torch.no_grad():\n",
    "      test_seq = X_all[:1]\n",
    "      preds = []\n",
    "      for _ in range(DAYS_TO_PREDICT):\n",
    "        y_test_pred = model(test_seq)\n",
    "        pred = torch.flatten(y_test_pred).item()\n",
    "        preds.append(pred)\n",
    "        new_seq = test_seq.numpy().flatten()\n",
    "        new_seq = np.append(new_seq, [pred])\n",
    "        new_seq = new_seq[1:]\n",
    "        test_seq = torch.as_tensor(new_seq).view(1, seq_len, 1).float()\n",
    "    predicted_cases = scaler.inverse_transform(np.expand_dims(preds, axis=0)).flatten()\n",
    "    predicted_index = pd.date_range(\n",
    "      start=daily_conf_cases.index[-1],\n",
    "      periods=DAYS_TO_PREDICT + 1,\n",
    "      closed='right'\n",
    "    )\n",
    "    predicted_cases = pd.Series(data=predicted_cases,index=predicted_index)\n",
    "    plt.plot(daily_conf_cases, label='Historical Daily Cases for ' + city)\n",
    "    plt.plot(predicted_cases, label='Predicted Daily Cases for ' + city)\n",
    "    plt.legend();\n",
    "    plt.savefig('model_output/'+ city + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cols=len(confirmed_df.axes[1])\n",
    "for x in range(1,total_cols) :\n",
    "    daily_conf_cases = confirmed_df.iloc[0:,x]\n",
    "    run_for_every_city(daily_conf_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 4.251269340515137 test loss: 127.43582153320312\n",
      "Epoch 10 train loss: 3.6781013011932373 test loss: 112.69070434570312\n",
      "Epoch 20 train loss: 3.5740504264831543 test loss: 107.01036071777344\n",
      "Epoch 30 train loss: 3.371168613433838 test loss: 55.20926284790039\n",
      "Epoch 40 train loss: 3.0252108573913574 test loss: 58.1048698425293\n",
      "Epoch 50 train loss: 2.3194124698638916 test loss: 72.12226867675781\n",
      "Epoch 60 train loss: 1.9592301845550537 test loss: 45.89516830444336\n",
      "Epoch 70 train loss: 1.8852767944335938 test loss: 57.142948150634766\n",
      "Epoch 80 train loss: 1.7876957654953003 test loss: 47.623504638671875\n",
      "Epoch 90 train loss: 2.1121959686279297 test loss: 51.62776184082031\n",
      "Epoch 0 train loss: 10.73287296295166\n",
      "Epoch 10 train loss: 7.356053829193115\n",
      "Epoch 20 train loss: 14.772233963012695\n",
      "Epoch 30 train loss: 7.484115123748779\n",
      "Epoch 40 train loss: 7.295403003692627\n",
      "Epoch 50 train loss: 6.2574663162231445\n",
      "Epoch 60 train loss: 5.665953159332275\n",
      "Epoch 70 train loss: 5.628957748413086\n",
      "Epoch 80 train loss: 3.1036837100982666\n",
      "Epoch 90 train loss: 2.8544797897338867\n",
      "Epoch 0 train loss: 2.8962104320526123 test loss: 32.47897720336914\n",
      "Epoch 10 train loss: 1.7277944087982178 test loss: 23.77791404724121\n",
      "Epoch 20 train loss: 1.698736548423767 test loss: 24.68792724609375\n",
      "Epoch 30 train loss: 1.592088222503662 test loss: 33.29909133911133\n",
      "Epoch 40 train loss: 1.1909425258636475 test loss: 26.681617736816406\n",
      "Epoch 50 train loss: 0.892313539981842 test loss: 27.289554595947266\n",
      "Epoch 60 train loss: 0.8954670429229736 test loss: 27.99970817565918\n",
      "Epoch 70 train loss: 0.7816252112388611 test loss: 25.555633544921875\n",
      "Epoch 80 train loss: 0.7720739245414734 test loss: 26.187471389770508\n",
      "Epoch 90 train loss: 0.7751062512397766 test loss: 29.12436866760254\n",
      "Epoch 0 train loss: 26.359651565551758\n",
      "Epoch 10 train loss: 12.722795486450195\n",
      "Epoch 20 train loss: 12.565336227416992\n",
      "Epoch 30 train loss: 12.481413841247559\n",
      "Epoch 40 train loss: 12.095973014831543\n",
      "Epoch 50 train loss: 12.69372844696045\n",
      "Epoch 60 train loss: 12.53474235534668\n",
      "Epoch 70 train loss: 12.709704399108887\n",
      "Epoch 80 train loss: 12.611367225646973\n",
      "Epoch 90 train loss: 12.474715232849121\n",
      "Epoch 0 train loss: 4.623269557952881 test loss: 527.6085815429688\n",
      "Epoch 10 train loss: 3.08052921295166 test loss: 489.3104248046875\n",
      "Epoch 20 train loss: 2.71260404586792 test loss: 375.6929931640625\n",
      "Epoch 30 train loss: 1.9190200567245483 test loss: 184.13885498046875\n",
      "Epoch 40 train loss: 1.7206236124038696 test loss: 151.84246826171875\n",
      "Epoch 50 train loss: 1.5421794652938843 test loss: 196.23382568359375\n",
      "Epoch 60 train loss: 1.4741499423980713 test loss: 174.55763244628906\n",
      "Epoch 70 train loss: 1.5182007551193237 test loss: 164.2718048095703\n",
      "Epoch 80 train loss: 1.5507471561431885 test loss: 170.2515106201172\n",
      "Epoch 90 train loss: 1.5030944347381592 test loss: 174.82131958007812\n",
      "Epoch 0 train loss: 10.39700698852539\n",
      "Epoch 10 train loss: 5.350655555725098\n",
      "Epoch 20 train loss: 8.053579330444336\n",
      "Epoch 30 train loss: 7.569168567657471\n",
      "Epoch 40 train loss: 3.585832118988037\n",
      "Epoch 50 train loss: 4.640368938446045\n",
      "Epoch 60 train loss: 2.4125170707702637\n",
      "Epoch 70 train loss: 1.9142887592315674\n",
      "Epoch 80 train loss: 1.6214207410812378\n",
      "Epoch 90 train loss: 1.318050742149353\n",
      "Epoch 0 train loss: 8.123031616210938 test loss: 5339.8759765625\n",
      "Epoch 10 train loss: 5.371035099029541 test loss: 5202.921875\n",
      "Epoch 20 train loss: 5.430479049682617 test loss: 5254.25244140625\n",
      "Epoch 30 train loss: 5.390015602111816 test loss: 5316.4443359375\n",
      "Epoch 40 train loss: 4.33327054977417 test loss: 5342.48876953125\n",
      "Epoch 50 train loss: 7.19923210144043 test loss: 4979.111328125\n",
      "Epoch 60 train loss: 5.625874042510986 test loss: 5266.71728515625\n",
      "Epoch 70 train loss: 5.387698173522949 test loss: 5457.921875\n",
      "Epoch 80 train loss: 5.4479265213012695 test loss: 5494.42724609375\n",
      "Epoch 90 train loss: 5.438604354858398 test loss: 5503.1630859375\n",
      "Epoch 0 train loss: 16.046340942382812\n",
      "Epoch 10 train loss: 11.376643180847168\n",
      "Epoch 20 train loss: 10.854632377624512\n",
      "Epoch 30 train loss: 11.043278694152832\n",
      "Epoch 40 train loss: 8.19300365447998\n",
      "Epoch 50 train loss: 6.240597248077393\n",
      "Epoch 60 train loss: 6.2546162605285645\n",
      "Epoch 70 train loss: 5.704768657684326\n",
      "Epoch 80 train loss: 3.7368855476379395\n",
      "Epoch 90 train loss: 2.3659913539886475\n",
      "Epoch 0 train loss: 11.305196762084961 test loss: 3108.68603515625\n",
      "Epoch 10 train loss: 6.84262752532959 test loss: 2972.6435546875\n",
      "Epoch 20 train loss: 6.809088230133057 test loss: 2975.95654296875\n",
      "Epoch 30 train loss: 6.838293552398682 test loss: 2949.771484375\n",
      "Epoch 40 train loss: 5.725983142852783 test loss: 2608.083740234375\n",
      "Epoch 50 train loss: 6.194123268127441 test loss: 2122.5771484375\n",
      "Epoch 60 train loss: 6.680145263671875 test loss: 3182.540283203125\n",
      "Epoch 70 train loss: 6.435003280639648 test loss: 2970.598388671875\n",
      "Epoch 80 train loss: 5.695319175720215 test loss: 2809.4794921875\n",
      "Epoch 90 train loss: 2.9860167503356934 test loss: 3640.398681640625\n",
      "Epoch 0 train loss: 11.317826271057129\n",
      "Epoch 10 train loss: 7.264331340789795\n",
      "Epoch 20 train loss: 3.7429592609405518\n",
      "Epoch 30 train loss: 1.8091529607772827\n",
      "Epoch 40 train loss: 1.3494036197662354\n",
      "Epoch 50 train loss: 1.1002053022384644\n",
      "Epoch 60 train loss: 1.3051444292068481\n",
      "Epoch 70 train loss: 0.9645865559577942\n",
      "Epoch 80 train loss: 0.937808096408844\n",
      "Epoch 90 train loss: 0.6457018256187439\n",
      "Epoch 0 train loss: 2.361521005630493 test loss: 1.2325024604797363\n",
      "Epoch 10 train loss: 2.0457043647766113 test loss: 0.4905831813812256\n",
      "Epoch 20 train loss: 1.773768424987793 test loss: 1.2305786609649658\n",
      "Epoch 30 train loss: 1.5554447174072266 test loss: 0.3754393756389618\n",
      "Epoch 40 train loss: 1.4801220893859863 test loss: 0.5969099998474121\n",
      "Epoch 50 train loss: 1.4268335103988647 test loss: 0.6025158762931824\n",
      "Epoch 60 train loss: 1.443596363067627 test loss: 0.6228500008583069\n",
      "Epoch 70 train loss: 1.4655208587646484 test loss: 0.6947146058082581\n",
      "Epoch 80 train loss: 1.3839600086212158 test loss: 0.7227798104286194\n",
      "Epoch 90 train loss: 1.3778024911880493 test loss: 0.6751312017440796\n",
      "Epoch 0 train loss: 2.943248748779297\n",
      "Epoch 10 train loss: 2.705277681350708\n",
      "Epoch 20 train loss: 2.677779197692871\n",
      "Epoch 30 train loss: 2.598893642425537\n",
      "Epoch 40 train loss: 2.511899471282959\n",
      "Epoch 50 train loss: 2.378836154937744\n",
      "Epoch 60 train loss: 2.280949592590332\n",
      "Epoch 70 train loss: 2.1951663494110107\n",
      "Epoch 80 train loss: 2.1696276664733887\n",
      "Epoch 90 train loss: 2.1790475845336914\n",
      "Epoch 0 train loss: 0.024331912398338318 test loss: 0.013427222147583961\n",
      "Epoch 10 train loss: 0.05345539003610611 test loss: 0.03355976566672325\n",
      "Epoch 20 train loss: 0.0009680231451056898 test loss: 0.0003894591354764998\n",
      "Epoch 30 train loss: 0.0002982759615406394 test loss: 0.00016152045282069594\n",
      "Epoch 40 train loss: 0.00040720539982430637 test loss: 0.00017888445290736854\n",
      "Epoch 50 train loss: 0.00032973464112728834 test loss: 0.00019512705330271274\n",
      "Epoch 60 train loss: 0.00038317590951919556 test loss: 0.00022487816750071943\n",
      "Epoch 70 train loss: 0.0003197502519469708 test loss: 0.00013467656390275806\n",
      "Epoch 80 train loss: 0.00025171355810016394 test loss: 0.00012247070844750851\n",
      "Epoch 90 train loss: 0.00018397928215563297 test loss: 0.00010893078433582559\n",
      "Epoch 0 train loss: 0.011745317839086056\n",
      "Epoch 10 train loss: 0.10194095224142075\n",
      "Epoch 20 train loss: 0.02750304527580738\n",
      "Epoch 30 train loss: 0.009598387405276299\n",
      "Epoch 40 train loss: 0.002854536985978484\n",
      "Epoch 50 train loss: 0.0007761261658743024\n",
      "Epoch 60 train loss: 0.00022095612075645477\n",
      "Epoch 70 train loss: 0.00011054697097279131\n",
      "Epoch 80 train loss: 0.0001009970364975743\n",
      "Epoch 90 train loss: 9.936529386322945e-05\n",
      "Epoch 0 train loss: 27.814075469970703 test loss: 350.0323486328125\n",
      "Epoch 10 train loss: 16.942516326904297 test loss: 313.69049072265625\n",
      "Epoch 20 train loss: 7.679191589355469 test loss: 225.0926055908203\n",
      "Epoch 30 train loss: 8.15324592590332 test loss: 259.067626953125\n",
      "Epoch 40 train loss: 7.5753302574157715 test loss: 234.3180694580078\n",
      "Epoch 50 train loss: 7.432373046875 test loss: 252.34140014648438\n",
      "Epoch 60 train loss: 7.437264919281006 test loss: 246.989013671875\n",
      "Epoch 70 train loss: 7.504449844360352 test loss: 255.80517578125\n",
      "Epoch 80 train loss: 7.1515045166015625 test loss: 256.0498046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 train loss: 7.446242332458496 test loss: 260.8211669921875\n",
      "Epoch 0 train loss: 18.76854133605957\n",
      "Epoch 10 train loss: 10.074174880981445\n",
      "Epoch 20 train loss: 9.218064308166504\n",
      "Epoch 30 train loss: 8.957684516906738\n",
      "Epoch 40 train loss: 10.983050346374512\n",
      "Epoch 50 train loss: 10.263555526733398\n",
      "Epoch 60 train loss: 9.620718002319336\n",
      "Epoch 70 train loss: 9.417646408081055\n",
      "Epoch 80 train loss: 9.158206939697266\n",
      "Epoch 90 train loss: 7.839008808135986\n",
      "Epoch 0 train loss: 20.59468650817871 test loss: 312.4052734375\n",
      "Epoch 10 train loss: 10.14889144897461 test loss: 193.70252990722656\n",
      "Epoch 20 train loss: 7.948349475860596 test loss: 19.655115127563477\n",
      "Epoch 30 train loss: 13.161867141723633 test loss: 70.18724060058594\n",
      "Epoch 40 train loss: 9.949705123901367 test loss: 211.75047302246094\n",
      "Epoch 50 train loss: 9.97968864440918 test loss: 197.6660919189453\n",
      "Epoch 60 train loss: 9.116024017333984 test loss: 115.96434020996094\n",
      "Epoch 70 train loss: 49.62544250488281 test loss: 20.754114151000977\n",
      "Epoch 80 train loss: 10.863851547241211 test loss: 250.288330078125\n",
      "Epoch 90 train loss: 10.727555274963379 test loss: 296.3848876953125\n",
      "Epoch 0 train loss: 27.565244674682617\n",
      "Epoch 10 train loss: 12.367947578430176\n",
      "Epoch 20 train loss: 11.006189346313477\n",
      "Epoch 30 train loss: 7.775425910949707\n",
      "Epoch 40 train loss: 12.95286750793457\n",
      "Epoch 50 train loss: 13.279221534729004\n",
      "Epoch 60 train loss: 12.00172233581543\n",
      "Epoch 70 train loss: 12.024755477905273\n",
      "Epoch 80 train loss: 11.947418212890625\n",
      "Epoch 90 train loss: 11.761850357055664\n",
      "Epoch 0 train loss: 5.735372066497803 test loss: 45.88650131225586\n",
      "Epoch 10 train loss: 4.009294033050537 test loss: 31.584970474243164\n",
      "Epoch 20 train loss: 3.935913324356079 test loss: 29.36726188659668\n",
      "Epoch 30 train loss: 3.891075372695923 test loss: 26.60539436340332\n",
      "Epoch 40 train loss: 2.717449903488159 test loss: 200.0819091796875\n",
      "Epoch 50 train loss: 4.0748090744018555 test loss: 31.617630004882812\n",
      "Epoch 60 train loss: 3.945819616317749 test loss: 30.856624603271484\n",
      "Epoch 70 train loss: 3.480346441268921 test loss: 19.77430534362793\n",
      "Epoch 80 train loss: 3.231794595718384 test loss: 16.005756378173828\n",
      "Epoch 90 train loss: 2.631835460662842 test loss: 262.280517578125\n",
      "Epoch 0 train loss: 8.317440032958984\n",
      "Epoch 10 train loss: 5.0218048095703125\n",
      "Epoch 20 train loss: 8.119114875793457\n",
      "Epoch 30 train loss: 5.836164951324463\n",
      "Epoch 40 train loss: 5.36262321472168\n",
      "Epoch 50 train loss: 5.0836381912231445\n",
      "Epoch 60 train loss: 3.629096269607544\n",
      "Epoch 70 train loss: 4.40234899520874\n",
      "Epoch 80 train loss: 3.624497175216675\n",
      "Epoch 90 train loss: 2.725029945373535\n",
      "Epoch 0 train loss: 7.621744632720947 test loss: 1819.93359375\n",
      "Epoch 10 train loss: 3.103487014770508 test loss: 1734.647705078125\n",
      "Epoch 20 train loss: 2.9670591354370117 test loss: 1731.7530517578125\n",
      "Epoch 30 train loss: 2.935861825942993 test loss: 1731.9862060546875\n",
      "Epoch 40 train loss: 2.9306187629699707 test loss: 1755.9219970703125\n",
      "Epoch 50 train loss: 2.9195122718811035 test loss: 1786.9378662109375\n",
      "Epoch 60 train loss: 2.87898325920105 test loss: 1856.049072265625\n",
      "Epoch 70 train loss: 2.8597216606140137 test loss: 2003.6781005859375\n",
      "Epoch 80 train loss: 2.8345022201538086 test loss: 2063.17529296875\n",
      "Epoch 90 train loss: 2.756309986114502 test loss: 2184.21484375\n",
      "Epoch 0 train loss: 7.0911335945129395\n",
      "Epoch 10 train loss: 4.035003662109375\n",
      "Epoch 20 train loss: 3.951758623123169\n",
      "Epoch 30 train loss: 3.6413064002990723\n",
      "Epoch 40 train loss: 5.7551350593566895\n",
      "Epoch 50 train loss: 4.614713668823242\n",
      "Epoch 60 train loss: 4.20414400100708\n",
      "Epoch 70 train loss: 4.149227619171143\n",
      "Epoch 80 train loss: 4.096105098724365\n",
      "Epoch 90 train loss: 4.114347457885742\n",
      "Epoch 0 train loss: 1.627798080444336 test loss: 12.250380516052246\n",
      "Epoch 10 train loss: 1.2847563028335571 test loss: 9.687027931213379\n",
      "Epoch 20 train loss: 1.256360411643982 test loss: 10.382966041564941\n",
      "Epoch 30 train loss: 1.2456550598144531 test loss: 10.29248332977295\n",
      "Epoch 40 train loss: 1.2066344022750854 test loss: 9.862346649169922\n",
      "Epoch 50 train loss: 1.1861233711242676 test loss: 10.861715316772461\n",
      "Epoch 60 train loss: 0.98236483335495 test loss: 12.856657981872559\n",
      "Epoch 70 train loss: 0.9574236869812012 test loss: 10.90161418914795\n",
      "Epoch 80 train loss: 0.956167459487915 test loss: 11.702691078186035\n",
      "Epoch 90 train loss: 0.9541975259780884 test loss: 11.150432586669922\n",
      "Epoch 0 train loss: 7.6629438400268555\n",
      "Epoch 10 train loss: 5.027112007141113\n",
      "Epoch 20 train loss: 4.010754108428955\n",
      "Epoch 30 train loss: 4.940335273742676\n",
      "Epoch 40 train loss: 4.888576030731201\n",
      "Epoch 50 train loss: 4.692302227020264\n",
      "Epoch 60 train loss: 3.4622485637664795\n",
      "Epoch 70 train loss: 4.095287322998047\n",
      "Epoch 80 train loss: 4.274539470672607\n",
      "Epoch 90 train loss: 3.5362861156463623\n",
      "Epoch 0 train loss: 1.5893224477767944 test loss: 73.03092956542969\n",
      "Epoch 10 train loss: 1.5168583393096924 test loss: 67.46629333496094\n",
      "Epoch 20 train loss: 1.4436976909637451 test loss: 70.08722686767578\n",
      "Epoch 30 train loss: 1.456540822982788 test loss: 70.33853149414062\n",
      "Epoch 40 train loss: 1.456360936164856 test loss: 70.07183837890625\n",
      "Epoch 50 train loss: 1.4487046003341675 test loss: 70.89260864257812\n",
      "Epoch 60 train loss: 1.4441702365875244 test loss: 72.18216705322266\n",
      "Epoch 70 train loss: 1.4780685901641846 test loss: 74.39801025390625\n",
      "Epoch 80 train loss: 1.4576395750045776 test loss: 78.54283142089844\n",
      "Epoch 90 train loss: 1.371577501296997 test loss: 123.0280532836914\n",
      "Epoch 0 train loss: 4.274043083190918\n",
      "Epoch 10 train loss: 3.6436517238616943\n",
      "Epoch 20 train loss: 3.0933945178985596\n",
      "Epoch 30 train loss: 3.667646646499634\n",
      "Epoch 40 train loss: 3.5837368965148926\n",
      "Epoch 50 train loss: 3.5169811248779297\n",
      "Epoch 60 train loss: 2.4664199352264404\n",
      "Epoch 70 train loss: 3.8771908283233643\n",
      "Epoch 80 train loss: 3.7572741508483887\n",
      "Epoch 90 train loss: 3.652740478515625\n",
      "Epoch 0 train loss: 7.8422698974609375 test loss: 1181.146728515625\n",
      "Epoch 10 train loss: 4.545928955078125 test loss: 1095.0946044921875\n",
      "Epoch 20 train loss: 4.497164726257324 test loss: 1039.003662109375\n",
      "Epoch 30 train loss: 30.743967056274414 test loss: 525.2736206054688\n",
      "Epoch 40 train loss: 5.119503974914551 test loss: 1218.5628662109375\n",
      "Epoch 50 train loss: 4.581716060638428 test loss: 1324.6865234375\n",
      "Epoch 60 train loss: 4.573123931884766 test loss: 1385.6737060546875\n",
      "Epoch 70 train loss: 4.604826927185059 test loss: 1432.3038330078125\n",
      "Epoch 80 train loss: 4.622210502624512 test loss: 1553.076171875\n",
      "Epoch 90 train loss: 4.478275299072266 test loss: 1722.7454833984375\n",
      "Epoch 0 train loss: 11.854220390319824\n",
      "Epoch 10 train loss: 7.374079704284668\n",
      "Epoch 20 train loss: 7.990726470947266\n",
      "Epoch 30 train loss: 7.298519134521484\n",
      "Epoch 40 train loss: 3.9723758697509766\n",
      "Epoch 50 train loss: 1.805628776550293\n",
      "Epoch 60 train loss: 1.2150095701217651\n",
      "Epoch 70 train loss: 1.241618037223816\n",
      "Epoch 80 train loss: 0.9237194657325745\n",
      "Epoch 90 train loss: 0.7427852749824524\n",
      "Epoch 0 train loss: 3.350477695465088 test loss: 887.261962890625\n",
      "Epoch 10 train loss: 2.3338069915771484 test loss: 792.7522583007812\n",
      "Epoch 20 train loss: 1.3792860507965088 test loss: 379.3155212402344\n",
      "Epoch 30 train loss: 2.600508451461792 test loss: 909.5478515625\n",
      "Epoch 40 train loss: 2.387491464614868 test loss: 977.9112548828125\n",
      "Epoch 50 train loss: 2.344867706298828 test loss: 1001.8533935546875\n",
      "Epoch 60 train loss: 2.3623385429382324 test loss: 988.360595703125\n",
      "Epoch 70 train loss: 2.297337055206299 test loss: 992.3033447265625\n",
      "Epoch 80 train loss: 2.2838196754455566 test loss: 984.31494140625\n",
      "Epoch 90 train loss: 2.161259651184082 test loss: 967.309814453125\n",
      "Epoch 0 train loss: 6.719193458557129\n",
      "Epoch 10 train loss: 2.7958767414093018\n",
      "Epoch 20 train loss: 1.3473950624465942\n",
      "Epoch 30 train loss: 1.1141127347946167\n",
      "Epoch 40 train loss: 1.139641523361206\n",
      "Epoch 50 train loss: 0.9935687780380249\n",
      "Epoch 60 train loss: 0.8706348538398743\n",
      "Epoch 70 train loss: 0.8888784050941467\n",
      "Epoch 80 train loss: 1.0216256380081177\n",
      "Epoch 90 train loss: 0.9366467595100403\n",
      "Epoch 0 train loss: 4.019609451293945 test loss: 287.2805480957031\n",
      "Epoch 10 train loss: 2.8028202056884766 test loss: 252.1717071533203\n",
      "Epoch 20 train loss: 2.5239617824554443 test loss: 110.38087463378906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 train loss: 2.0937769412994385 test loss: 288.65093994140625\n",
      "Epoch 40 train loss: 2.159374952316284 test loss: 690.2090454101562\n",
      "Epoch 50 train loss: 2.1374330520629883 test loss: 998.4773559570312\n",
      "Epoch 60 train loss: 2.090484619140625 test loss: 1251.9188232421875\n",
      "Epoch 70 train loss: 2.070995569229126 test loss: 1370.14794921875\n",
      "Epoch 80 train loss: 2.030294895172119 test loss: 1461.480712890625\n",
      "Epoch 90 train loss: 2.018136978149414 test loss: 1622.0836181640625\n",
      "Epoch 0 train loss: 12.372042655944824\n",
      "Epoch 10 train loss: 6.418564796447754\n",
      "Epoch 20 train loss: 2.258286714553833\n",
      "Epoch 30 train loss: 1.4186627864837646\n",
      "Epoch 40 train loss: 1.2889403104782104\n",
      "Epoch 50 train loss: 1.833652138710022\n",
      "Epoch 60 train loss: 1.3374217748641968\n",
      "Epoch 70 train loss: 1.1314557790756226\n",
      "Epoch 80 train loss: 1.1179494857788086\n",
      "Epoch 90 train loss: 0.922927737236023\n",
      "Epoch 0 train loss: 18.939828872680664 test loss: 325.8379211425781\n",
      "Epoch 10 train loss: 7.669549465179443 test loss: 219.1395721435547\n",
      "Epoch 20 train loss: 7.4470391273498535 test loss: 201.49581909179688\n",
      "Epoch 30 train loss: 6.682346820831299 test loss: 34.126319885253906\n",
      "Epoch 40 train loss: 8.593791007995605 test loss: 183.37525939941406\n",
      "Epoch 50 train loss: 7.370721340179443 test loss: 208.10719299316406\n",
      "Epoch 60 train loss: 7.231755256652832 test loss: 181.96817016601562\n",
      "Epoch 70 train loss: 6.956422328948975 test loss: 136.8949432373047\n",
      "Epoch 80 train loss: 7.044823169708252 test loss: 127.81561279296875\n",
      "Epoch 90 train loss: 7.3171162605285645 test loss: 187.4334716796875\n",
      "Epoch 0 train loss: 27.54479217529297\n",
      "Epoch 10 train loss: 13.995646476745605\n",
      "Epoch 20 train loss: 6.421037197113037\n",
      "Epoch 30 train loss: 20.67943572998047\n",
      "Epoch 40 train loss: 19.21965217590332\n",
      "Epoch 50 train loss: 14.789306640625\n",
      "Epoch 60 train loss: 14.138509750366211\n",
      "Epoch 70 train loss: 13.398638725280762\n",
      "Epoch 80 train loss: 4.347224235534668\n",
      "Epoch 90 train loss: 18.760007858276367\n",
      "Epoch 0 train loss: 1.0405272245407104 test loss: 27.526973724365234\n",
      "Epoch 10 train loss: 1.0328654050827026 test loss: 28.018476486206055\n",
      "Epoch 20 train loss: 1.0443768501281738 test loss: 28.29182243347168\n",
      "Epoch 30 train loss: 1.0272212028503418 test loss: 27.57929229736328\n",
      "Epoch 40 train loss: 1.039595603942871 test loss: 27.85495948791504\n",
      "Epoch 50 train loss: 1.030908226966858 test loss: 27.73444366455078\n",
      "Epoch 60 train loss: 1.0411021709442139 test loss: 19.803871154785156\n",
      "Epoch 70 train loss: 0.8965936303138733 test loss: 16.351533889770508\n",
      "Epoch 80 train loss: 0.901971697807312 test loss: 26.230825424194336\n",
      "Epoch 90 train loss: 1.0602279901504517 test loss: 67.01813507080078\n",
      "Epoch 0 train loss: 2.831921100616455\n",
      "Epoch 10 train loss: 2.821735143661499\n",
      "Epoch 20 train loss: 2.7773447036743164\n",
      "Epoch 30 train loss: 2.735525369644165\n",
      "Epoch 40 train loss: 2.148864507675171\n",
      "Epoch 50 train loss: 2.803616762161255\n",
      "Epoch 60 train loss: 2.816063404083252\n",
      "Epoch 70 train loss: 2.813722848892212\n",
      "Epoch 80 train loss: 2.794705629348755\n",
      "Epoch 90 train loss: 2.782827377319336\n",
      "Epoch 0 train loss: 12.66071891784668 test loss: 213.3138427734375\n",
      "Epoch 10 train loss: 6.755701065063477 test loss: 160.71832275390625\n",
      "Epoch 20 train loss: 2.4078915119171143 test loss: 11.646845817565918\n",
      "Epoch 30 train loss: 5.736189365386963 test loss: 48.6551628112793\n",
      "Epoch 40 train loss: 2.4409537315368652 test loss: 10.31645393371582\n",
      "Epoch 50 train loss: 1.5357122421264648 test loss: 35.77067184448242\n",
      "Epoch 60 train loss: 1.009294033050537 test loss: 132.92820739746094\n",
      "Epoch 70 train loss: 0.8443851470947266 test loss: 146.73374938964844\n",
      "Epoch 80 train loss: 0.7914952039718628 test loss: 134.51205444335938\n",
      "Epoch 90 train loss: 0.6729276180267334 test loss: 101.75350189208984\n",
      "Epoch 0 train loss: 34.225982666015625\n",
      "Epoch 10 train loss: 18.999895095825195\n",
      "Epoch 20 train loss: 17.439943313598633\n",
      "Epoch 30 train loss: 17.378047943115234\n",
      "Epoch 40 train loss: 17.29227638244629\n",
      "Epoch 50 train loss: 16.59592628479004\n",
      "Epoch 60 train loss: 18.415637969970703\n",
      "Epoch 70 train loss: 17.627944946289062\n",
      "Epoch 80 train loss: 17.510103225708008\n",
      "Epoch 90 train loss: 17.066932678222656\n",
      "Epoch 0 train loss: 4.565442085266113 test loss: 191.6305694580078\n",
      "Epoch 10 train loss: 2.5181796550750732 test loss: 151.09739685058594\n",
      "Epoch 20 train loss: 2.061901569366455 test loss: 30.176612854003906\n",
      "Epoch 30 train loss: 1.6164844036102295 test loss: 334.9417419433594\n",
      "Epoch 40 train loss: 0.7405219078063965 test loss: 2000.246826171875\n",
      "Epoch 50 train loss: 0.5862444043159485 test loss: 3164.022216796875\n",
      "Epoch 60 train loss: 0.6731195449829102 test loss: 3357.938232421875\n",
      "Epoch 70 train loss: 0.6410642862319946 test loss: 3232.18408203125\n",
      "Epoch 80 train loss: 0.6082723140716553 test loss: 3121.30126953125\n",
      "Epoch 90 train loss: 0.580329418182373 test loss: 3021.245361328125\n",
      "Epoch 0 train loss: 25.435327529907227\n",
      "Epoch 10 train loss: 13.630383491516113\n",
      "Epoch 20 train loss: 8.751673698425293\n",
      "Epoch 30 train loss: 16.148645401000977\n",
      "Epoch 40 train loss: 14.402859687805176\n",
      "Epoch 50 train loss: 14.166664123535156\n",
      "Epoch 60 train loss: 14.075958251953125\n",
      "Epoch 70 train loss: 13.87121868133545\n",
      "Epoch 80 train loss: 13.323312759399414\n",
      "Epoch 90 train loss: 15.217240333557129\n",
      "Epoch 0 train loss: 3.1062989234924316 test loss: 49.22885513305664\n",
      "Epoch 10 train loss: 2.3364944458007812 test loss: 40.768402099609375\n",
      "Epoch 20 train loss: 2.2840802669525146 test loss: 43.01747131347656\n",
      "Epoch 30 train loss: 2.883803129196167 test loss: 26.342084884643555\n",
      "Epoch 40 train loss: 2.220665454864502 test loss: 43.72121810913086\n",
      "Epoch 50 train loss: 2.0307788848876953 test loss: 46.530548095703125\n",
      "Epoch 60 train loss: 2.2053370475769043 test loss: 47.067604064941406\n",
      "Epoch 70 train loss: 2.1852264404296875 test loss: 45.8061637878418\n",
      "Epoch 80 train loss: 2.1166181564331055 test loss: 48.3411979675293\n",
      "Epoch 90 train loss: 2.079610586166382 test loss: 53.763702392578125\n",
      "Epoch 0 train loss: 15.628103256225586\n",
      "Epoch 10 train loss: 10.126636505126953\n",
      "Epoch 20 train loss: 9.917367935180664\n",
      "Epoch 30 train loss: 9.838921546936035\n",
      "Epoch 40 train loss: 9.591373443603516\n",
      "Epoch 50 train loss: 12.426423072814941\n",
      "Epoch 60 train loss: 10.911992073059082\n",
      "Epoch 70 train loss: 9.857013702392578\n",
      "Epoch 80 train loss: 9.872737884521484\n",
      "Epoch 90 train loss: 9.914294242858887\n",
      "Epoch 0 train loss: 9.979594230651855 test loss: 853.7108764648438\n",
      "Epoch 10 train loss: 4.9328932762146 test loss: 753.1489868164062\n",
      "Epoch 20 train loss: 4.681286811828613 test loss: 590.2947387695312\n",
      "Epoch 30 train loss: 4.750669479370117 test loss: 348.326171875\n",
      "Epoch 40 train loss: 4.816178798675537 test loss: 636.1302490234375\n",
      "Epoch 50 train loss: 4.4145097732543945 test loss: 600.7904663085938\n",
      "Epoch 60 train loss: 9.490885734558105 test loss: 379.2868347167969\n",
      "Epoch 70 train loss: 5.371115684509277 test loss: 704.0166015625\n",
      "Epoch 80 train loss: 4.541988372802734 test loss: 598.5565185546875\n",
      "Epoch 90 train loss: 4.413649559020996 test loss: 557.5460205078125\n",
      "Epoch 0 train loss: 19.270925521850586\n",
      "Epoch 10 train loss: 11.039076805114746\n",
      "Epoch 20 train loss: 10.97702693939209\n",
      "Epoch 30 train loss: 5.324305534362793\n",
      "Epoch 40 train loss: 3.8093883991241455\n",
      "Epoch 50 train loss: 1.7534904479980469\n",
      "Epoch 60 train loss: 1.3057289123535156\n",
      "Epoch 70 train loss: 1.0231088399887085\n",
      "Epoch 80 train loss: 0.8725178837776184\n",
      "Epoch 90 train loss: 0.7479347586631775\n",
      "Epoch 0 train loss: 4.3374457359313965 test loss: 70.60659790039062\n",
      "Epoch 10 train loss: 3.3141417503356934 test loss: 60.32452392578125\n",
      "Epoch 20 train loss: 3.348158359527588 test loss: 63.988040924072266\n",
      "Epoch 30 train loss: 3.1719019412994385 test loss: 90.31427764892578\n",
      "Epoch 40 train loss: 3.282954216003418 test loss: 81.0115966796875\n",
      "Epoch 50 train loss: 3.306959629058838 test loss: 61.10970687866211\n",
      "Epoch 60 train loss: 3.292509078979492 test loss: 62.26316833496094\n",
      "Epoch 70 train loss: 3.2811501026153564 test loss: 65.66234588623047\n",
      "Epoch 80 train loss: 3.217142105102539 test loss: 76.25028228759766\n",
      "Epoch 90 train loss: 3.0671286582946777 test loss: 119.39825439453125\n",
      "Epoch 0 train loss: 16.819551467895508\n",
      "Epoch 10 train loss: 8.085845947265625\n",
      "Epoch 20 train loss: 13.990852355957031\n",
      "Epoch 30 train loss: 9.583070755004883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 train loss: 8.746350288391113\n",
      "Epoch 50 train loss: 5.303770542144775\n",
      "Epoch 60 train loss: 5.829370021820068\n",
      "Epoch 70 train loss: 3.3525729179382324\n",
      "Epoch 80 train loss: 2.36708664894104\n",
      "Epoch 90 train loss: 2.4373672008514404\n",
      "Epoch 0 train loss: 20.026954650878906 test loss: 1257.0009765625\n",
      "Epoch 10 train loss: 10.564562797546387 test loss: 1011.1482543945312\n",
      "Epoch 20 train loss: 3.5138888359069824 test loss: 451.9433288574219\n",
      "Epoch 30 train loss: 9.770078659057617 test loss: 761.3167114257812\n",
      "Epoch 40 train loss: 4.040634632110596 test loss: 530.6218872070312\n",
      "Epoch 50 train loss: 2.02760648727417 test loss: 728.2655639648438\n",
      "Epoch 60 train loss: 1.4785369634628296 test loss: 790.4743041992188\n",
      "Epoch 70 train loss: 1.2494949102401733 test loss: 891.045654296875\n",
      "Epoch 80 train loss: 1.0289682149887085 test loss: 896.6912841796875\n",
      "Epoch 90 train loss: 0.973457396030426 test loss: 869.7395629882812\n",
      "Epoch 0 train loss: 26.016077041625977\n",
      "Epoch 10 train loss: 14.717351913452148\n",
      "Epoch 20 train loss: 26.95171546936035\n",
      "Epoch 30 train loss: 17.952129364013672\n",
      "Epoch 40 train loss: 16.643720626831055\n",
      "Epoch 50 train loss: 15.41611099243164\n",
      "Epoch 60 train loss: 19.24321746826172\n",
      "Epoch 70 train loss: 15.928110122680664\n",
      "Epoch 80 train loss: 15.539719581604004\n",
      "Epoch 90 train loss: 14.156475067138672\n",
      "Epoch 0 train loss: 32.37640380859375 test loss: 12.513842582702637\n",
      "Epoch 10 train loss: 15.923587799072266 test loss: 5.196565628051758\n",
      "Epoch 20 train loss: 5.530927658081055 test loss: 4.889927387237549\n",
      "Epoch 30 train loss: 2.4165070056915283 test loss: 0.718590259552002\n",
      "Epoch 40 train loss: 1.92037034034729 test loss: 1.5959467887878418\n",
      "Epoch 50 train loss: 1.8404362201690674 test loss: 1.0570048093795776\n",
      "Epoch 60 train loss: 1.8072247505187988 test loss: 1.3603335618972778\n",
      "Epoch 70 train loss: 1.769182562828064 test loss: 1.2508958578109741\n",
      "Epoch 80 train loss: 1.7792129516601562 test loss: 1.2722630500793457\n",
      "Epoch 90 train loss: 1.7485789060592651 test loss: 1.316993236541748\n",
      "Epoch 0 train loss: 49.493186950683594\n",
      "Epoch 10 train loss: 3.0779900550842285\n",
      "Epoch 20 train loss: 2.9118521213531494\n",
      "Epoch 30 train loss: 3.0395774841308594\n",
      "Epoch 40 train loss: 2.9709367752075195\n",
      "Epoch 50 train loss: 2.955876588821411\n",
      "Epoch 60 train loss: 2.8778984546661377\n",
      "Epoch 70 train loss: 2.9243361949920654\n",
      "Epoch 80 train loss: 2.99304461479187\n",
      "Epoch 90 train loss: 2.925246477127075\n"
     ]
    }
   ],
   "source": [
    "total_cols=len(confirmed_df.axes[1])\n",
    "for x in range(15,total_cols) :\n",
    "    daily_conf_cases = confirmed_df.iloc[0:,x]\n",
    "    daily_conf_cases.index = pd.to_datetime(confirmed_df['Date'])\n",
    "    run_model(daily_conf_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
